stage1:
  epochs: 1
  learning_rate: 2.0e-05
  warmup_steps: 2000
  max_steps: 12000
  mixer: topk
stage2:
  epochs: 1
  learning_rate: 1.5e-05
  warmup_steps: 1000
  max_steps: 12000
  mixer: topk
stage3:
  epochs: 1
  learning_rate: 1.0e-05
  warmup_steps: 500
  max_steps: 8000
  mixer: topk
curriculum:
  order: ["stage1", "stage2", "stage3"]
  eval_interval_steps: 2000
  checkpoint_interval_steps: 1000
  gradient_accumulation_steps: 8
  global_batch_tokens: 1048576
global:
  weight_decay: 0.1
  grad_clip: 1.0
  logging_steps: 50
  save_steps: 1000
  eval_batch_size: 4
  train_batch_size_per_gpu: 2
  seed: 42
  use_flash_attention: true
  gradient_checkpointing: true
  resume_from: ""
  output_dir: "/mnt/checkpoints/omni_stack_moe"
