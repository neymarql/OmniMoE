warmup:
  epochs: 1
  learning_rate: 2.0e-04
  warmup_steps: 500
  max_steps: 5000
  # Branch-specific router schedules (fallback to router_temperature if omitted)
  router_temperature_projector: { start: 1.6, end: 1.1, steps: 5000 }
  router_temperature_llm:       { start: 1.5, end: 1.1, steps: 5000 }
  router_temperature_vision:    { start: 1.4, end: 1.1, steps: 5000 }
  # Branch-specific jitter schedules or constants (fallback to router_jitter_std if omitted)
  router_jitter_projector: { start: 0.02, end: 0.01, steps: 5000 }
  router_jitter_llm:       { start: 0.02, end: 0.01, steps: 5000 }
  router_jitter_vision:    { start: 0.015, end: 0.01, steps: 5000 }
  router_temperature: { start: 1.5, end: 1.1, steps: 5000 }
  router_jitter_std: 0.02
  freeze:
    llm: ["all"]
    vision: ["all_but_moe"]
    projector: []
  projector_layers: 2
align:
  epochs: 1
  learning_rate: 1.0e-04
  warmup_steps: 1000
  max_steps: 12000
  router_temperature_projector: { start: 1.1, end: 1.0, steps: 12000 }
  router_temperature_llm:       { start: 1.1, end: 1.0, steps: 12000 }
  router_temperature_vision:    { start: 1.1, end: 1.0, steps: 12000 }
  router_jitter_projector: 0.01
  router_jitter_llm:       0.01
  router_jitter_vision:    0.01
  router_temperature: { start: 1.1, end: 1.0, steps: 12000 }
  router_jitter_std: 0.01
  freeze:
    llm: ["bottom_24"]
    vision: ["first_half"]
    projector: []
  projector_layers: 3
reasoning:
  epochs: 2
  learning_rate: 5.0e-05
  warmup_steps: 500
  max_steps: 6000
  router_temperature_projector: { start: 1.0, end: 1.0, steps: 6000 }
  router_temperature_llm:       { start: 1.0, end: 1.0, steps: 6000 }
  router_temperature_vision:    { start: 1.0, end: 1.0, steps: 6000 }
  router_jitter_projector: 0.0
  router_jitter_llm:       0.0
  router_jitter_vision:    0.0
  router_temperature: { start: 1.0, end: 1.0, steps: 6000 }
  router_jitter_std: 0.0
  freeze:
    llm: []
    vision: []
    projector: []
  projector_layers: 4
curriculum:
  order: ["warmup", "align", "reasoning"]
  eval_interval_steps: 2000
  checkpoint_interval_steps: 1000
  gradient_accumulation_steps: 8
  global_batch_tokens: 1048576
global:
  weight_decay: 0.1
  grad_clip: 1.0
  logging_steps: 50
  save_steps: 1000
  eval_batch_size: 4
  train_batch_size_per_gpu: 2
  seed: 42
  dispatch_profile: false
  use_flash_attention: true
  gradient_checkpointing: true
  resume_from: ""
  output_dir: "/mnt/checkpoints/omni_stack_moe"
  aux_loss_coef:
    text_moe: 0.02
    vision_moe: 0.03
    projector_moe: 0.05
